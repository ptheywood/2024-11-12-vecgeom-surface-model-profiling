---
title: "VecGeom `surface_model` profiling"
author: 
  - "Peter Heywood, Research Software Engineer"
institute: 
  - "The University of Sheffield"
date: "2024-11-12"
logo: ./img/UOSLogo_Primary_Violet_RGB.svg
footer: "VecGeom `surface_model` profiling - [SWIFT-HEP #8 Joint with ExaTEPP](https://indico.cern.ch/event/1466097/)"

format:
  revealjs:
    theme: theme/tuos.scss
    embed-resources: true  # only enable when publish-ready for perf reasons
    template-partials: 
      - title-slide.html
    # show-notes: separate-page
    slide-number: c
    width: 1050
    height: 700
    margin: 0.1
    min-scale: 0.2
    max-scale: 2.0
    auto-stretch: false
    fontsize: 32px
    navigation-mode: linear
    controls: true
    mouse-wheel: true
    include-after-body: 
      - text: |
          <script type="text/javascript">
          Reveal.addEventListener('slidechanged', (event) => {
            const isSnOn = (event.currentSlide.dataset.hideSlideNumber !== 'true');
            Reveal.configure({ slideNumber: isSnOn ? "c" : isSnOn});
          });
          </script>

# title slide background
title-slide-attributes:
  data-background-color: "#9ADBE8" # $tuos-powder-blue
  # data-background-color: "#D7F1F6" # $tuos-powder-blue-40

---

# VecGeom `surface_model` & `testRaytracing` {.divider .teal visibility="uncounted" data-hide-slide-number='true'}

<!-- Only 20 mins. -->

## VecGeom `surface_model`

> VecGeom is a geometry modeller library with hit-detection features as needed by particle detector simulation at the LHC and beyond

- [gitlab.cern.ch/VecGeom/VecGeom](https://gitlab.cern.ch/VecGeom/VecGeom)
- CPU & GPU implementations 
- Solid modelling / representation
  - Not ideal for GPU
- Developers are adding a Surface modelling / representation
  - [`surface_model` branch](https://gitlab.cern.ch/VecGeom/VecGeom/-/tree/surface_model) 
    - Similarities with [Orange](https://celeritas-project.github.io/celeritas/user/implementation/orange.html)/[Celeritas](https://github.com/celeritas-project/celeritas)

## `testRaytracing`

::: {.smaller}

- Loads geometry, generates random rays, tests on CPU and GPU.
  - Solid and Surface representation
  - Validation
  - Without/with BVH
  - GPU Surface BVH with multiple kernel launches & split kernels

:::

::: {.smaller}

- `test/surfaces/testRaytracing.{h/cpp/cu}`

:::

::: {.smaller}

- Profiling tweaks:
  - [NVTX](https://nvidia.github.io/NVTX/) ranges for profile annotation
  - `-oncpu 0` to disable cpu runs to to speed up profiling

:::

```{.bash .slightly-bigger}
testRaytracing -gdml_name file.gdml -ongpu 1 -mmunit 0.1 -verbosity 0 \
    -accept_zeros 1 -validate_results 1 -nrays 524228 -use_TB_gun 0   \
    -only_surf 0 -test_bvh 1 -bvh_single_step 1 -bvh_split_step 1     \
    -oncpu 1
```

## `testRaytracing` timeline CPU & GPU {.smalltitle}

![CPU & GPU Timeline (TBHGCal, `65536` rays, `use_TB_gun 0`, V100) ](img/nsys-overveiws/nvtx-timeline-with-cpu-tbhgcal-65536-gun-0.png){fig-alt="Annotated NSight Systems timeline for CPU & GPU testRaytracing for TBHGCal with 65536 rays and -use_TB_gun 0" width="100%" height="100%" }

![CPU & GPU Timeline (TBHGCal, `16384` rays, `use_TB_gun 1`, V100)](img/nsys-overveiws/nvtx-timeline-with-cpu-tbhgcal-16384-gun-1.png){fig-alt="Annotated NSight Systems timeline for CPU & GPU testRaytracing for TBHGCal with 16348 rays and -use_TB_gun 1" width="100%" height="100%"}

<!-- 
```bash
testRaytracing -gdml_name TBHGCal.gdml -ongpu 1 -mmunit 0.1 -verbosity 0     \
    -accept_zeros 1 -validate_results 1 -nrays 65536 -use_TB_gun 0       \
    -only_surf 0 -test_bvh 1 -bvh_single_step 1 -bvh_split_step 1 -oncpu 1

testRaytracing -gdml_name TBHGCal.gdml -ongpu 1 -mmunit 0.1 -verbosity 0     \
    -accept_zeros 1 -validate_results 1 -nrays 16384 -use_TB_gun 1       \
    -only_surf 0 -test_bvh 1 -bvh_single_step 1 -bvh_split_step 1 -oncpu 1

```
-->

## `testRaytracing` timeline GPU surface-only {.smalltitle}

![GPU & surface only timeline (TBHGCal, `524228` rays, `use_tb_gun 1`, V100)](img/nsys-overveiws/nvtx-timeline-gpu-only-tbhgcal-524228-tbgun-1.png){fig-alt="Annotated NSight Systems timeline for CPU & GPU testRaytracing for TBHGCal with 524228 rays and -use_TB_gun 0" width="100%" height="100%"}

```bash
testRaytracing -gdml_name TBHGCal.gdml -ongpu 1 -mmunit 0.1 -verbosity 0   \
    -accept_zeros 1 -validate_results 0 -nrays 524228  -use_TB_gun 1       \
    -only_surf 1 -test_bvh 1 -bvh_single_step 1 -bvh_split_step 1 -oncpu 0
```

## Hardware & Geometries

::: {.center layout="[[1], [-1], [1]]"}

| GPU       | CC | CPU                  | Cluster                             | Driver       |
|:----------|----|:---------------------|:------------------------------------|:-------------|
| V100 SXM2 | 70 | Intel Xeon Gold 6138 | [TUoS Bessemer][bessemer-gpu-specs] | `550.127.05` |
| A100 SXM4 | 80 | AMD EPYC 7413        | [TUoS Stanage][stanage-gpu-specs]   | `550.127.05` |
| H100 PCIe | 90 | AMD EPYC 7413        | [TUoS Stanage][stanage-gpu-specs]   | `550.127.05` |
| GH200     | 90 | Nvidia Grace         | [N8CIR Bede][bede-gpu-specs]        | `560.35.03`  |

: {.striped .table-full-width}

| Geometry                                                                 | Touchables |
|:-------------------------------------------------------------------------|-----------:|
| [`trackML.gdml`][trackML.gdml]                                           |    `18790` |
| [`TBHGCal181Oct_fixdup.gdml`][TBHGCal181Oct_fixdup.gdml]                 |    `61802` |
| [`cms2026D110DD4hep_fix.gdml`][cms2026D110DD4hep_fix.gdml]               | `13133900` |
| [`LHCb_Upgrade_onlyECALandHCAL.gdml`][LHCb_Upgrade_onlyECALandHCAL.gdml] | `18429884` |

: {.striped .table-full-width}

:::

## Initial Benchmarking

<!-- @todo - move to backup? -->
<!-- @todo - adjust labels? -->

:::: {.columns}

::: {.column .center .even-smaller style="padding-top: 120px; width: 15%"}

- 3 geometries
- 10 million rays 
  - not using TB gun
  - TBHGCal unrealistic
- 3 machines
  - A100
  - H100 pcie
  - GH200

:::

::: {.column width="85%"}

![Initial `testRaytracing` benchmarking with 10 Million Rays](img/benchmark-figures/vecgeom-surface-8ce55c5c-original-benchmark-10Mrays.png){fig-alt="Initial testRaytracing benchmarking" width="100%" height="100%"}

:::

::::

# Surface modelling approaches {.divider .teal visibility="uncounted" data-hide-slide-number='true'}

## `PropagateRaysSurf`

@todo - timeline

@todo - benchmark data for propagateRaysSurf

- Single kernel launch for the full batch of rays.
- Grid-stride loop over each input ray
  - Runs until the ray is outside

## `PropagateRaysSurfBVH`

@todo - timeline, compared to before
@todo - benchmark data for propagateRaysSurfBVH, compared to before.

- Same as `PropagateRaysSurf`, but using a *Bounding Volume Hierarchy* (BVH)
  - via `vgbrep::protonav::BVHSurfNavigator<Real_t>`
- More efficient finding of ray-surface intersection

## Nvidia GPU Structure

- NVIDIA GPUs are made up of many *Streaming Multiprocessors* (SMs)
  - GH100 die contains 144 SMs

![Full GH100.<br /> © NVIDIA Corporation ([source](https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/))](img/nvidia/Full-H100-GPU-with-144-SMs-1024x457.png)

## Nvidia Streaming Multiprocessor (SM)

::::: {.columns}

:::: {.column width=60%}

::: {}

- Each SM contains 
  - Compute units (Int32, FP32, FP64)
  - Register file (64K 32-bit registers for Hopper) 
  - Instruction Cache, L1 Caches & Shared memory

:::

::: {}

- L2 Cache & Global memory are outside the SM
  - i.e. higher latency

:::

::::

:::: {.column width=40%}

![GH100 SM.<br /> © NVIDIA Corporation ([source](https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/))](img/nvidia/H100-Streaming-Multiprocessor-SM-625x869.png)

::::

:::::

## Occupancy: CUDA background

:::: {.smaller}

<!-- @todo - thread block diagram from phd? -->

::: {}

- Function executed on the GPU by many threads is a **kernel**
- Threads organised as a **grid** of (*clusters of*) **blocks** of **threads**

:::

::: {}

- Blocks are issued to an SM, and they become **resident**
  - Remain on the SM until all threads returns
  - Limit on number of resident threads per SM due to hardware constraints
    - `2048` threads per SM on most x100 GPUs

:::

::: {}

- A **warp** is the group of `32` threads which execute in lock-step 
  - **Active warps** are resident in an SM and have not executed their last instruction
  - **Eligible warps** are ready to issue their next instruction
  - **Stalled warps** are not ready for their next instruction
    - due to resources, latency, etc

:::

::::

## Occupancy

::: {.smaller}

- **Occupancy** - Ratio of active warps on an SM to the maximum it could support
  - **Theoretical Occupancy** - Occupancy based on hardware and kernel constraints
  - **Achieved Occupancy** - the actual ratio at execution time
    - `achieved <= theoretical` deu to stalls, workload imbalance, etc

:::

::: {.smaller}

- Higher Occupancy does not guarantee higher performance
  - **But** low occupancy reduces ability to hide latency
  - **And** cannot use all resources if it is too low

:::

![](img/ncu/v100-ncu-occupancy-PropagateRaysSurf-gpu-only-tbhgcal-524228-tbgun-1.png){fig-alt="Nsight Compute Occupancy for PropagateRaysSurf kernel on V100" width=100%}

![](img/ncu/v100-ncu-occupancy-PropagateRaysSurfBVH-gpu-only-tbhgcal-524228-tbgun-1.png){fig-alt="Nsight Compute Occupancy for PropagateRaysSurfBVH kernel on V100" width=100%}


## Occupancy: registers per thread

- Optimizer selects the number of **registers per thread** for a kernel at compile time
  - Hardware maximum of `255` 32-bit registers per thread in recent HPC GPUs 
  - Can tell the compiler the upper limit for a given kernel, or all kernels
  - If there are not enough registers they **spill** to **Local Memory**
    - **Local Memory** is a special region of **Global Memory** - i.e. high latency
      - (and terribly named)

- Large complex kernels often use more registers
  - But optimisers work in mysterious ways
- FP64 uses more 32-bit registers than FP32...

![](img/ncu/v100-ncu-occupancy-PropagateRaysSurf-gpu-only-tbhgcal-524228-tbgun-1.png){fig-alt="Nsight Compute Occupancy for PropagateRaysSurf kernel on V100" width=100%}

![](img/ncu/v100-ncu-occupancy-PropagateRaysSurfBVH-gpu-only-tbhgcal-524228-tbgun-1.png){fig-alt="Nsight Compute Occupancy for PropagateRaysSurfBVH kernel on V100" width=100%}

- @todo - spills measurement?

## Stalls

- @todo - ncu screenshots showing stall reasons, mostly L2 storeboard stalls

- High register counts & spills
- Not surprising for FP64 workloads

- Increasing occupancy does not guarantee improved performance
- But it can enable the *warp scheduler* to context switch to other threads within the block 
  - **hiding latency** 

<!-- 
## Streaming Multiprocessor, Residency, Occupancy, Registers file

- Nvidia GPUs contain many (GH100 has `144`) Streaming Multiprocessors (SMs). 
- @todo nv SM diagram? https://resources.nvidia.com/en-us-tensor-core/gtc22-whitepaper-hopper
- Each SM has a fixed number of 32 bit registers per SM (64K 32-bit registers for Hopper)
- Each thread has a upper limit of 255 registers / thread
- The required number of register per thread limits the number of threads which can become resident within an SM

- **Occupancy** Ratio of active warps on an SM to the maximum it could support
- Increasing occupancy does not guarantee improved performance
- But it can enable the *warp scheduler* to context switch to other threads within the block, *hiding latency* 

- *resident threads* have been loaded into an SM
- *active threads of a warp* are participating in the current instruction
- *disabled threads of a warp* are not participating in the current instruction
- achieved occupancy will be lower than theoretical occupancy, especially when the schedulers cannot balance the work. 
- *eligable warps* are ready to isseu their next instruction (instruction has been fetched, execution unit(s) for instruction area ready, no unmet dependencies)
- *stalled warps* are not ready.
- Ideally want atleas as many eligable warps as schdulers per SM at any one point.
- 2 warps per SM is not enough...  [source](https://docs.nvidia.com/gameworks/content/developertools/desktop/analysis/report/cudaexperiments/kernellevel/issueefficiency.htm)
- If the kernel requires more than 255 registers per thread, registers will spill to *local memory*
- *local memory* is a (terribly named) region of *global memory* (with L1 caching of stores) - i.e. it is outside of the SM so latency is bad. -->

## `PropagateRaysSurfBVHSingle`

<!-- After occupancy was identified, severin implemented some altenative kernels to try and improve performance -->

- Split the single kernel launch into a loop of 2 kernels:
  - `PropagateRaysSurfBVHSingle` - traverse a single step
  - `filterAliveRays` - compact the alive/inside rays for the next iteration
- Reduces register pressure ?@todo

- @todo - timeline screen shot
- @todo - compare for a single model? 
- @todo - Only able to get partial data so far
- @todo - show register use,  runtime.

## `ComputeNextStepAndNextSurfaces` + `RelocateToNextVolumes`

- Splits  `PropagateRaysSurfBVHSingle` into `ComputeStepAndNextSurfaces` & `RelocateToNextVolumes`
- @todo - timeline screenshot + comparison for the same geometry
- @todo - show register usage + total time.
- @todo - benchmark plot? 
- @todo - Only able to get partial data so far


## Limit registers per thread

- Use `--ptxas-options=-v` to get register use and spill count at compile time
  - Or it is reported by the profiler(s)
- Globally via `--maxrregcount N`
- Per kernel:
  - `__maxnreg__` for CUDA >= 12.4
  - `__launch_bounds__` for older CUDAs, but less intuitive

- I.e. `cmake .. -DCMAKE_CUDA_FLAGS="--maxrregcount 128 -Xptxas -v" ...`

- Increases occupancy
- Increases (forces) register spilling
- Often hurts more than it helps

- @todo - figures and data if possible.


## Increased block size

- `testRaytracing.cu` uses a fixed number of threads per block of `32`
  - Larger values would allow more latency hiding within a block
    - But SMs can contain threads from separate blocks, so it might not make a difference.

```{.cpp}
// testRaytracing.cu
  constexpr int initThreads = 32;
  int initBlocks            = (nrays + initThreads - 1) / initThreads;
```

- Alternatively, could use a per-kernel occupancy API method to maximise occupancy
  - i.e. `cudaOccupancyMaxPotentialBlockSize`
  - Specialises for the target GPU architecture.
  - Note: if kernel is overloaded, must specify the correct function ptr

```{.cpp}
  int minGridSize = 0;
  int blockSize = 0;
  int gridSize = 0;
  // ...
  cudaOccupancyMaxPotentialBlockSize(&minGridSize, &blockSize, PropagateRaysSurf);
  gridSize = (nrays + blockSize - 1) / blockSize;
  PropagateRaysSurf<<<gridSize, blockSize>>>(nrays, ...);
```


## Increased block size

- @todo - results from changing block size. 
- @todo - V100 only, impact may vary on diff GPUs
- @todo - selected block size for each kernel via occupancy API for FP64 and fp32
- @todo - any changes
- @todo - may be worth trying again with lower reg count?

## Mixed precision mode

::::: {layout="[[1], [-1], [1]]"}

:::: {.columns}

::: {.column .smaller width="60%"}

<!-- + Reduce register pressure by using single precision in for some but not all real values -->
- Single precision for some but not all `Real`
- Reduces register pressure
- More FP32 units
  - 2:1 on most X100/X200 cards
  - 32:1 or 64:1 on most other NVIDIA GPUs
- Less memory to move

:::

::: {.column width="40%"}

```{.cpp .bigger style="margin-top: 0px; overflow: hidden;"}
// testRaytracing.h
using Real_t = float;
```

:::

::::

:::: {.columns}

::: {.column .smaller width="60%"}

- *"not stable on most geometries"*
  - Assertions triggered by many geometries
  - Launch failures
  - `bvh_single_step` and `bvh_split_step` run indefinitely
    - For some geometries, and on some GPUs?
  - Unintentionally created a 1.3TB log file...

:::

::: {.column width="40%"}

```{.console .bigger style="margin-top: 285px"}
$ du -sh slurm-842405.out
1.3T    slurm-842405.out
```

:::

::::

:::::

## Impact of mixed precision

- Less data to read, move and store
- More FP32 units
- Reduces Register pressure
  - @todo - number.

- @todo - change in runtime not significant for the base kernel


<!-- 

# Benchmarking {.divider .teal visibility="uncounted" data-hide-slide-number='true'} - @todo - include this as extra slides if figures generated?

## Per Model or Per kernel?

- @todo decide. Maybe only show a little bit of data / include gaps?

## TrackML

## LHcb Update

## CMS2026

## HGCal

@todo -->

# Thank you {.divider .flamingo visibility="uncounted" data-hide-slide-number='true'}

# Additional Slides {.divider .coral visibility="uncounted" data-hide-slide-number='true'}

## CMake Configuration

```bash
cmake -S . -B build \ 
      -DCMAKE_BUILD_TYPE=Release \
      -DCMAKE_CUDA_ARCHITECTURES="70;80;90" \
      -DVECGEOM_ENABLE_CUDA=ON -DVECGEOM_GDML=ON \
      -DBACKEND=Scalar -DVECGEOM_USE_NAVTUPLE=ON \
      -DVECGEOM_BVH_SINGLE=ON -DVECGEOM_BUILTIN_VECCORE=ON
```

## Surface model construction timeline

::: {.smaller .no-caption-p-margin}

![`trackML.gdml` ](img/nsys-overveiws/nvtx-timeline-gh200-trackML-500k.png){fig-alt="trackML.gdml" width="100%"}

![`TBHGCal181Oct_fixdup.gdml`](img/nsys-overveiws/nvtx-timeline-gh200-tbhgcal-500k.png){fig-alt="TBHGCal181Oct_fixdup.gdml" width="100%"}

![`LHCb_Upgrade_onlyECALandHCAL.gdml`](img/nsys-overveiws/nvtx-timeline-gh200-lhcb-500k.png){fig-alt="LHCb_Upgrade_onlyECALandHCAL.gdml" width="100%"}

![`cms2026D110DD4hep_fix.gdml`](img/nsys-overveiws/nvtx-timeline-gh200-cms-500k.png){fig-alt="cms2026D110DD4hep_fix.gdml" width="100%"}

:::

::: {.smaller}

- Larger/more complex geometries would benefit from solid -> surface conversion optimisation
- GH200
- `524228` rays
- `FP64`

:::


<!-- ## Hardware 

::: {.center layout="[[-1], [1], [-1]]"}

| GPU       | CC | CPU                  | Cluster                             | Driver       |
|:----------|----|:---------------------|:------------------------------------|:-------------|
| V100 SXM2 | 70 | Intel Xeon Gold 6138 | [TUoS Bessemer][bessemer-gpu-specs] | `550.127.05` |
| A100 SXM4 | 80 | AMD EPYC 7413        | [TUoS Stanage][stanage-gpu-specs]   | `550.127.05` |
| H100 PCIe | 90 | AMD EPYC 7413        | [TUoS Stanage][stanage-gpu-specs]   | `550.127.05` |
| GH200     | 90 | Nvidia Grace         | [N8CIR Bede][bede-gpu-specs]        | `560.35.03`  |

: {.striped .table-full-width}

::: -->


<!-- ## Geometries

::: {.center layout="[[-1], [1], [-1]]"}

| Geometry                                                                 | Touchables |
|:-------------------------------------------------------------------------|-----------:|
| [`trackML.gdml`][trackML.gdml]                                           |    `18790` |
| [`TBHGCal181Oct_fixdup.gdml`][TBHGCal181Oct_fixdup.gdml]                 |    `61802` |
| [`cms2026D110DD4hep_fix.gdml`][cms2026D110DD4hep_fix.gdml]               | `13133900` |
| [`LHCb_Upgrade_onlyECALandHCAL.gdml`][LHCb_Upgrade_onlyECALandHCAL.gdml] | `18429884` |

: {.striped .table-full-width}

::: -->


<!-- ## Long Tail

- @todo Slide showing the long tail? -->



<!-- 
# Templates {.divider .flamingo visibility="uncounted" data-hide-slide-number='true'}


::: {.plug}

Sample plug 

[https://github.com/ptheywood](https://github.com/ptheywood)

:::


## Screnshot slide {.smalltitle}

![](img/image.png){fig-alt="alt text" width="100%" height="100%"}

## columns 


:::: {.columns}

::: {.column width="50%"}

- column
- one

```{.bash}
#! /usr/bin/env bash
echo "regular"
```

:::

::: {.column width="50%"}

- column
- two

```{.bash .bigger}
#! /usr/bin/env bash
echo "bigger"
```

:::

:::: -->

<!-- Reference links -->
[bessemer-gpu-specs]: https://docs.hpc.shef.ac.uk/en/latest/bessemer/cluster_specs.html#gpu-node-specifications
[stanage-gpu-specs]: https://docs.hpc.shef.ac.uk/en/latest/stanage/cluster_specs.html#gpu-nodes
[bede-gpu-specs]: https://bede-documentation.readthedocs.io/en/latest/hardware/index.html
[trackML.gdml]: https://gitlab.cern.ch/VecGeom/VecGeom/-/blob/master/test/gdml/gdmls/trackML.gdml?ref_type=heads
[TBHGCal181Oct_fixdup.gdml]: https://cern-my.sharepoint.com/:u:/g/personal/severin_diederichs_cern_ch/EWFFpWCW4GpOhJgQfBaZTVkBjsjHke93QO7Em3TrK_GkFg
[cms2026D110DD4hep_fix.gdml]: https://cern-my.sharepoint.com/:u:/g/personal/severin_diederichs_cern_ch/EfTefx_Me71Gs37_h-nuaTQB_8lqNsCMSz9kNmQOpEl2sA
[LHCb_Upgrade_onlyECALandHCAL.gdml]: https://cern-my.sharepoint.com/:u:/g/personal/severin_diederichs_cern_ch/EWdDg95YWNNOuznQ9DnrfMUBmQrT9Akp2ixUPBuhGIdQww