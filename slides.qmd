---
title: "VecGeom `surface_model` profiling"
author: 
  - "Peter Heywood, Research Software Engineer"
institute: 
  - "The University of Sheffield"
date: "2024-11-12"
logo: ./img/UOSLogo_Primary_Violet_RGB.svg
footer: "VecGeom `surface_model` profiling - [SWIFT-HEP #8 Joint with ExaTEPP](https://indico.cern.ch/event/1466097/)"

format:
  revealjs:
    theme: theme/tuos.scss
    embed-resources: true  # only enable when publish-ready for perf reasons
    template-partials: 
      - title-slide.html
    # show-notes: separate-page
    slide-number: c
    width: 1050
    height: 700
    margin: 0.1
    min-scale: 0.2
    max-scale: 2.0
    auto-stretch: false
    fontsize: 32px
    navigation-mode: linear
    controls: true
    mouse-wheel: true
    include-after-body: 
      - text: |
          <script type="text/javascript">
          Reveal.addEventListener('slidechanged', (event) => {
            const isSnOn = (event.currentSlide.dataset.hideSlideNumber !== 'true');
            Reveal.configure({ slideNumber: isSnOn ? "c" : isSnOn});
          });
          </script>

# title slide background
title-slide-attributes:
  data-background-color: "#9ADBE8" # $tuos-powder-blue
  # data-background-color: "#D7F1F6" # $tuos-powder-blue-40

---

# VecGeom `surface_model` & `testRaytracing` {.divider .teal visibility="uncounted" data-hide-slide-number='true'}

<!-- Only 20 mins. -->

## VecGeom `surface_model`

> VecGeom is a geometry modeller library with hit-detection features as needed by particle detector simulation at the LHC and beyond

- [gitlab.cern.ch/VecGeom/VecGeom](https://gitlab.cern.ch/VecGeom/VecGeom)
- CPU & GPU implementations 
- Solid modelling / representation
  - Not ideal for GPU
- Developers are adding a Surface modelling / representation
  - [`surface_model` branch](https://gitlab.cern.ch/VecGeom/VecGeom/-/tree/surface_model) 
    - Similarities with [Orange](https://celeritas-project.github.io/celeritas/user/implementation/orange.html)/[Celeritas](https://github.com/celeritas-project/celeritas)

## `testRaytracing`

::: {.smaller}

- Loads geometry, generates random rays, tests on CPU and GPU.
  - Solid and Surface representation
  - Validation
  - With & Without BVH
  - GPU Surface BVH using multiple kernel launches & split kernels

:::

::: {.smaller}

- `test/surfaces/testRaytracing.{h/cpp/cu}`

:::

::: {.smaller}

- Profiling tweaks:
  - [NVTX](https://nvidia.github.io/NVTX/) ranges for profile annotation
  - `-oncpu 0` to disable cpu runs to to speed up profiling

:::

<!-- ```{.bash .slightly-bigger}
testRaytracing -gdml_name file.gdml -ongpu 1 -mmunit 0.1 -verbosity 0 \
    -accept_zeros 1 -validate_results 1 -nrays 524228 -use_TB_gun 0   \
    -only_surf 0 -test_bvh 1 -bvh_single_step 1 -bvh_split_step 1     \
    -oncpu 1
``` -->

## `testRaytracing` timeline CPU & GPU {.smalltitle}

<!-- ![CPU & GPU Timeline (TBHGCal, `65536` rays, `-use_TB_gun 0`, V100) ](img/nsys/nvtx-timeline-with-cpu-tbhgcal-65536-gun-0.png){fig-alt="Annotated NSight Systems timeline for CPU & GPU testRaytracing for TBHGCal with 65536 rays and -use_TB_gun 0" width="100%" height="100%" } -->

<br />

![CPU & GPU Timeline (TBHGCal, `16384` rays, `-use_TB_gun 1`, V100)](img/nsys/nvtx-timeline-with-cpu-tbhgcal-16384-gun-1.png){fig-alt="Annotated NSight Systems timeline for CPU & GPU testRaytracing for TBHGCal with 16348 rays and -use_TB_gun 1" width="100%" height="100%"}

<!-- 
```bash
testRaytracing -gdml_name TBHGCal.gdml -ongpu 1 -mmunit 0.1 -verbosity 0 \
    -accept_zeros 1 -validate_results 1 -nrays 65536 -use_TB_gun 0       \
    -only_surf 0 -test_bvh 1 -bvh_single_step 1 -bvh_split_step 1 -oncpu 1
```
-->

```bash
testRaytracing -gdml_name TBHGCal.gdml -ongpu 1 -mmunit 0.1 -verbosity 0 \
    -accept_zeros 1 -validate_results 1 -nrays 16384 -use_TB_gun 1       \
    -only_surf 0 -test_bvh 1 -bvh_single_step 1 -bvh_split_step 1 -oncpu 1
```

## `testRaytracing` timeline `-oncpu 0 -only_surf 1` {.smalltitle}

<br />

![GPU & surface only timeline (TBHGCal, `524228` rays, `-use_tb_gun 1`, V100)](img/nsys/nvtx-timeline-gpu-only-tbhgcal-524228-tbgun-1.png){fig-alt="Annotated NSight Systems timeline for CPU & GPU testRaytracing for TBHGCal with 524228 rays and -use_TB_gun 0" width="100%" height="100%"}

<br />

```bash
testRaytracing -gdml_name TBHGCal.gdml -ongpu 1 -mmunit 0.1 -verbosity 0   \
    -accept_zeros 1 -validate_results 0 -nrays 524228  -use_TB_gun 1       \
    -only_surf 1 -test_bvh 1 -bvh_single_step 1 -bvh_split_step 1 -oncpu 0
```

## Hardware & Geometries

::: {.center layout="[[1], [-1], [1]]"}

| GPU       | CC | CPU                  | Cluster                             | Driver       |
|:----------|----|:---------------------|:------------------------------------|:-------------|
| V100 SXM2 | 70 | Intel Xeon Gold 6138 | [TUoS Bessemer][bessemer-gpu-specs] | `550.127.05` |
| A100 SXM4 | 80 | AMD EPYC 7413        | [TUoS Stanage][stanage-gpu-specs]   | `550.127.05` |
| H100 PCIe | 90 | AMD EPYC 7413        | [TUoS Stanage][stanage-gpu-specs]   | `550.127.05` |
| GH200     | 90 | Nvidia Grace         | [N8CIR Bede][bede-gpu-specs]        | `560.35.03`  |

: {.striped .table-full-width}

| Geometry                                                                 | Touchables |
|:-------------------------------------------------------------------------|-----------:|
| [`trackML.gdml`][trackML.gdml]                                           |    `18790` |
| [`TBHGCal181Oct_fixdup.gdml`][TBHGCal181Oct_fixdup.gdml]                 |    `61802` |
| [`cms2026D110DD4hep_fix.gdml`][cms2026D110DD4hep_fix.gdml]               | `13133900` |
| [`LHCb_Upgrade_onlyECALandHCAL.gdml`][LHCb_Upgrade_onlyECALandHCAL.gdml] | `18429884` |

: {.striped .table-full-width}

:::

## Initial Benchmarking

:::: {.columns}

::: {.column .center .even-smaller style="padding-top: 120px; width: 15%"}

- 3 geometries
- 10 million rays 
  - not using TB gun
  - TBHGCal unrealistic
- 3 machines
  - A100
  - H100 pcie
  - GH200

:::

::: {.column width="85%"}

![Initial `testRaytracing` benchmarking with 10 Million Rays](img/benchmark-figures/vecgeom-surface-8ce55c5c-original-benchmark-10Mrays.png){fig-alt="Initial testRaytracing benchmarking" width="100%" height="100%"}

:::

::::

# Initial surface model profiling {.divider .teal visibility="uncounted" data-hide-slide-number='true'}

## `PropagateRaysSurf` & `PropagateRaysSurfBVH`

:::{.smaller}

- Single kernel launch for the full batch of rays.
- Grid-stride over rays, steps until the ray is outside the geometry
- *Bounding Volume Hierarchy (BVH)* improves work-efficiency
- TBHGCal, `-nrays 524228 -use_TB_gun 1` on V100

:::

![](img/nsys/per-kernel-zoom-traverse-surface-vs-bvh-tbhgcal-500k-v100-all.png){fig-alt="PropagateRaysSurf abd PropagateRaysSurfBVH nsight systems timelines compared for 524228 rays of use_TB_gun for TBHGCal. Showing the single kernel launch for each kernel" width=100%}


:::{.somewhat-smaller .absolute top=480 right=0}

| Method                 | Duration (s) |
|:-----------------------|-------------:|
| `PropagateRaysSurf`    |     `11.454` |
| `PropagateRaysSurfBVH` |     ` 3.428` |

:::

## `PropagateRaysSurf` & `PropagateRaysSurfBVH` kernel profiling

::: {.smaller}

- `ncu --set full -o report.ncu-rep ./testRaytracing ...`
- For all 4 geometries on V100 and GH200 highlighted issues are:
  - Very low occupancy
  - Long scoreboard (memory) stalls
  - Scattered memory access

:::

:::{style="text-align: center"}

![](img/ncu/v100-tbhgcal-500k-propagaterayssurf-occupancy-table.png){fig-alt="V100 TBHGCal PropagateraysSurf Occupancy Table" height=100px}
![](img/ncu/v100-tbhgcal-500k-propagaterayssurf-stalls.png){fig-alt="V100 TBHGCal PropagateraysSurf Stalls" height=200px}

:::

## Nvidia GPU Structure

::: {.smaller}

- NVIDIA GPUs are made up of many *Streaming Multiprocessors* (SMs)
  - GH100 die contains 144 SMs (8 GPCs of 18 SMs)

:::

![Full GH100.<br /> © NVIDIA Corporation ([source](https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/))](img/nvidia/Full-H100-GPU-with-144-SMs-1024x457.png)

## Nvidia Streaming Multiprocessor (SM)

::::: {.columns}

:::: {.column width=60%}

::: {.smaller}

- Each SM contains:
  - Compute units (Int32, FP32, FP64)
  - **Register file (64K 32-bit registers for Hopper)**
  - Instruction Cache
  - L1 Caches & Shared memory

:::

::: {.smaller}

- Latency to Resources outside the SM is higher
  - L2 Cache 
  - Global memory (and local memory)

:::

::::

:::: {.column width=40%}

![GH100 SM.<br /> © NVIDIA Corporation ([source](https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/))](img/nvidia/H100-Streaming-Multiprocessor-SM-625x869.png)

::::

:::::

## CUDA Terminology

:::: {.smaller}

::: {}

- *Kernels* are executed by  **grid** of (*clusters of*) **blocks** of **threads**

:::

::: {}

- Blocks are issued to an SM, and they become **resident**
  - Remain on the SM until all threads in the block return

:::

::: {}

- A **warp** is the group of `32` threads which execute in lock-step 
  - **Active warps** are resident in an SM and have not executed their last instruction
  <!-- - **Eligible warps** are ready to issue their next instruction -->
  - **Stalled warps** are not ready to execute their next instruction
  - **Maximum number of resident warps & threads per SM**
    - `64` warps, `2048` threads for V100

:::

::::

## Occupancy

::: {.smaller}

- **Occupancy** - ratio of *active warps* on an SM to the *maximum warps per SM*
  - **Theoretical Occupancy** - occupancy based on hardware and kernel constraints
  - **Achieved Occupancy** - observed occupancy during execution

:::

::: {.smaller .bg-darker}

- Low occupancy 
  - Reduces latency hiding (i.e. *long scoreboard stalls*)
  - Cannot exploit all of the GPU if too low
- Higher occupancy does not guarantee higher performance

:::

::: {.smaller}
- Theoretical Occupancy can be limited by
  - **Registers per thread**
  - Threads per Block
  - Shared Memory per thread
:::

## Occupancy: registers per thread

:::{.smaller}

- **Registers per thread** for kernel selected by optimiser at compile time
  - Maximum of `255` 32-bit registers per thread in recent HPC GPUs
  - 64K 32-bit registers per SM
  - `PropagateRaysSurf` uses 255 reg/thread: `12.5%` occupancy 

:::

:::{style="text-align: center"}

![](./img/ncu/v100-tbhgcal-500k-propagaterayssurf-occupancy-graph.png){fig-caption="PropagateRaysSurf Occupancy register-per-thread graph for 524228 ray TBHGCal on V100" width="100%"}

:::

:::{.smaller}
- Attempt to improve by:

  1. Split monotlithic kernel
  2. Force the compiler to use less registers per thread
  3. Lower precision reals

:::


# Optimisation attempts {.divider .teal visibility="uncounted" data-hide-slide-number='true'}


## Split monolithinc kernel: `-bvh_single_step`

:::{.somewhat-smaller}

- Split the single kernel launch into a loop of 2 kernels:
  - `PropagateRaysSurfBVHSingle` - traverse a single step
  - `filterAliveRays` - compact the alive/inside rays for the next iteration
:::

![](img/nsys/per-kernel-zoom-single-step-zoom-500k-v100-all.png){fig-alt="Zoomed in timeline view of the bvh_single_step method for 524228 rays of TBHGCal on V100"}

:::{.somewhat-smaller .list-style-none}

- 🎉 `250` registers per thread 
- 😞 Still `12.5%` occupancy
- 😄 Shorter runtime
- ❌ Some runtime errors on some hardware to debug

:::

:::{.somewhat-smaller .absolute top=480 right=0}

| Method                 | Duration (s)     |
|:-----------------------|-----------------:|
| `PropagateRaysSurf`    |         `11.454` |
| `PropagateRaysSurfBVH` |         ` 3.428` |
| `bvh_single_step`      |     **` 2.261`** |

:::

## Split further:  `-bvh_split_step`


:::{.somewhat-smaller}

- Splits `PropagateRaysSurfBVHSingle` into
  - `ComputeStepAndNextSurfaces`
  - `RelocateToNextVolumes`
<!-- - Unstable for some Geometries on GH200? -->
:::

![](img/nsys/per-kernel-zoom-split-step-very-zoomed-500k-v100-all.png){fig-alt="Very Zoomed in timeline view of the bvh_split_step method for 524228 rays of TBHGCal on V100"}

:::{.somewhat-smaller .list-style-none}

- 🎉 `153` reg / thread for `ComputeStepAndNextSurfaces`
  - 😄 `18.75%` occupancy
- 🎉 `218` reg / thread for `RelocateToNextVolumes`
  - 😞 Still `12.5%` occupancy
- 😄 Shorter runtime
- ❌ Some runtime errors on some hardware to debug

:::

:::{.somewhat-smaller .absolute top=480 right=0}

| Method                 | Duration (s)     |
|:-----------------------|-----------------:|
| `PropagateRaysSurf`    |         `11.454` |
| `PropagateRaysSurfBVH` |         ` 3.428` |
| `bvh_single_step`      |         ` 2.261` |
| `bvh_split_step`       |     **` 1.948`** |

:::

## Increase Occupancy: Set maximum registers per thread

::::{.columns}

:::{.column .somewhat-smaller width=50%}

- Force NVCC to limit registers per thread
  - For all kernels via `--maxrregcount=N`
  - Per kernel via qualifiers
    - `__maxnreg__` for CUDA >= 12.4
    - `__launch_bounds__` (less intuitive)
:::

:::{.column .somewhat-smaller width=50%}

- Often hurts more than it helps
  - Increased Occupancy
  - Forces register spills to high-latency local memory 

:::

::::

<hr />

::::{.columns}

:::{.column .somewhat-smaller width=50%}

- TBHGCal, `-nrays 524228`, `-use_TB_gun 1` , V100
- `-maxrregcount=128`
- `25%` theoretical occupancy
- ~75% increase in global memory transfer for `PropagateRaysSurfBVH`

:::

:::{.column .somewhat-smaller width=50%}

| Strategy               | Reference  | 128reg/thread |
|------------------------|---------------:|-----------:|
| `PropagateRaysSurf`    |       `11.456` | **`9.224`**| 
| `PropagateRaysSurfBVH` |       **`3.430`** | `3.515` |
| `bvh_single_step`      |       **`2.263`** | `2.323` |
| `bvh_split_step`       |       **`1.948`** | `2.145` |


:::

::::

:::{style="text-align: center"}

![](img/ncu/v100-tbhgcal-500k-propagaterayssurf-128-occupancy-graph.png){fig-caption="PropagateRaysSurf Occupancy register-per-thread graph for 524228 ray TBHGCal on V100 when forced to 128 reg per thread" height="150px"}

:::

## Increase Occupancy:  Set maximum registers per thread

![Surface approach runtimes for `-nrays 524228` on V100 with maximum register counts of 255 and 128](img/v100-maxregcount.png){width=80%}


## Mixed precision mode

::::: {layout="[[1], [1]]"}

:::: {.columns}

::: {.column .smaller width="60%"}

<!-- + Reduce register pressure by using single precision in for some but not all real values -->
- Single precision for some but not all `Real`
- Reduces register pressure
- Reduces volume of data movement
- 2 FP32 for each FP64 unit on HPC GPUs
  - 32:1 or 64:1 on most other NVIDIA GPUs

:::

::: {.column width="40%"}

```{.cpp style="margin-top: 8px; overflow: hidden;"}
// testRaytracing.h
using Real_t = float;
```

:::

::::

::: {.smaller}

- ❌ *"not stable on most geometries"*
  - Assertions triggered by many geometries
  - Launch failures, incomplete profile reports
  - `bvh_single_step` and `bvh_split_step` run indefinitely
    - For some geometries, some of the time

:::

:::::

## Unsuccessful FP32 runs on V100

:::{.somewhat-smaller .list-style-none}
- ❌ Very few successful runs / configurations

:::

:::::{.columns}

::::{.column .smaller width="60%"}


![Very partial FP64 vs FP32 results](img/v100-f32-partial-results.png){width=100%}

::::


::::{.column width="40%"}

:::{.somewhat-smaller}

| Precision | Reg/Thread | LHCb `524228` rays| TBHGCal `500` rays
|-----------|------------|-------------|
| FP64 | 255 | 243ms | 301ms |
| FP32 | 208 | 298ms | 295ms |

: `ProjectRaysSurf` on V100

<hr />

+ Unintentionally created a 1.3TB log file...

:::

```{.console style="margin-top: 20px"}
$ du -sh slurm-842405.out
1.3T    slurm-842405.out
```

::::

:::::

:::{.smaller}

- Needs investigation

:::

## Increased block size

:::{.smaller}

- `testRaytracing.cu` uses a fixed number of threads per block of `32`
  - Different block sizes may impact performance

:::

:::{.smaller}

```{.cpp}
// testRaytracing.cu
  constexpr int initThreads = 32;
  int initBlocks            = (nrays + initThreads - 1) / initThreads;
```

:::{.smaller}

- Alternatively, can use a per-kernel occupancy API method to maximise occupancy
  - i.e. `cudaOccupancyMaxPotentialBlockSize`
  - Specialises for the target GPU architecture.
  <!-- - Note: if kernel is overloaded, must specify the correct function ptr -->

:::

```{.cpp}
  int minGridSize = 0;
  int blockSize = 0;
  int gridSize = 0;
  // ...
  cudaOccupancyMaxPotentialBlockSize(&minGridSize, &blockSize, PropagateRaysSurf);
  gridSize = (nrays + blockSize - 1) / blockSize;
  PropagateRaysSurf<<<gridSize, blockSize>>>(nrays, ...);
```
:::

## Increased block size

:::{.smaller}

- `trackML.gdml`, `524228` rays, V100

| Strategy               | Reference&nbsp;Time(s) | Time(s) | Selected Blocksizes   |
|------------------------|-----------------------:|--------:|:----------------------|
| `PropagateRaysSurf`    |                `2.006` | `2.206` | `256`                 |
| `PropagateRaysSurfBVH` |                `0.215` | `0.229` | `256`                 |
| `bvh_single_step`      |                `0.127` | `0.131` | `256` & `1024`        |
| `bvh_split_step`       |                `0.116` | `0.174` | `384`, `256` & `1024` |

:::

:::{.smaller}

- Not an improvement on V100
  - Try on other architectures?
  - Try other values between `32` and `256`?

:::

# Thank you {.divider .flamingo visibility="uncounted" data-hide-slide-number='true'}

# Additional Slides {.divider .coral visibility="uncounted" data-hide-slide-number='true'}

## CMake Configuration

```bash
cmake -S . -B build \ 
      -DCMAKE_BUILD_TYPE=Release \
      -DCMAKE_CUDA_ARCHITECTURES="70;80;90" \
      -DVECGEOM_ENABLE_CUDA=ON -DVECGEOM_GDML=ON \
      -DBACKEND=Scalar -DVECGEOM_USE_NAVTUPLE=ON \
      -DVECGEOM_BVH_SINGLE=ON -DVECGEOM_BUILTIN_VECCORE=ON
```


## Surface model construction timeline

::: {.smaller .no-caption-p-margin}

![`trackML.gdml` ](img/nsys/nvtx-timeline-gh200-trackML-500k.png){fig-alt="trackML.gdml" width="100%"}

![`TBHGCal181Oct_fixdup.gdml`](img/nsys/nvtx-timeline-gh200-tbhgcal-500k.png){fig-alt="TBHGCal181Oct_fixdup.gdml" width="100%"}

![`LHCb_Upgrade_onlyECALandHCAL.gdml`](img/nsys/nvtx-timeline-gh200-lhcb-500k.png){fig-alt="LHCb_Upgrade_onlyECALandHCAL.gdml" width="100%"}

![`cms2026D110DD4hep_fix.gdml`](img/nsys/nvtx-timeline-gh200-cms-500k.png){fig-alt="cms2026D110DD4hep_fix.gdml" width="100%"}

:::

::: {.smaller}

- Larger/more complex geometries would benefit from solid -> surface conversion optimisation
- `524228` rays on GH200 in FP64

:::

## Workload imbalance

![PM Sampling report showing workload imbalance for LHCb with `524228` rays on GH200 ](./img/ncu/gh200-lhcb-pm-sampling-long-tail.png){width="100%"}


<!-- Reference links -->
[bessemer-gpu-specs]: https://docs.hpc.shef.ac.uk/en/latest/bessemer/cluster_specs.html#gpu-node-specifications
[stanage-gpu-specs]: https://docs.hpc.shef.ac.uk/en/latest/stanage/cluster_specs.html#gpu-nodes
[bede-gpu-specs]: https://bede-documentation.readthedocs.io/en/latest/hardware/index.html
[trackML.gdml]: https://gitlab.cern.ch/VecGeom/VecGeom/-/blob/master/test/gdml/gdmls/trackML.gdml?ref_type=heads
[TBHGCal181Oct_fixdup.gdml]: https://cern-my.sharepoint.com/:u:/g/personal/severin_diederichs_cern_ch/EWFFpWCW4GpOhJgQfBaZTVkBjsjHke93QO7Em3TrK_GkFg
[cms2026D110DD4hep_fix.gdml]: https://cern-my.sharepoint.com/:u:/g/personal/severin_diederichs_cern_ch/EfTefx_Me71Gs37_h-nuaTQB_8lqNsCMSz9kNmQOpEl2sA
[LHCb_Upgrade_onlyECALandHCAL.gdml]: https://cern-my.sharepoint.com/:u:/g/personal/severin_diederichs_cern_ch/EWdDg95YWNNOuznQ9DnrfMUBmQrT9Akp2ixUPBuhGIdQww